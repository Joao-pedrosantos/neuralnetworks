{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53a477b",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANNs) Project 3\n",
    "\n",
    "Made by: João Pedro Santos, Matheus Castellucci, Rodrigo Medeiros "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sj49f5vx4r",
   "source": "## Introdução\n\n### O que é Stable Diffusion?\n\nStable Diffusion é um modelo de geração de imagens baseado em **difusão latente** (Latent Diffusion Model - LDM), desenvolvido pela Stability AI. Diferentemente de modelos como DALL-E, o Stable Diffusion é open-source e pode ser executado em hardware mais acessível.\n\n### Como Funciona?\n\nO processo de geração segue estas etapas:\n\n1. **Codificação de Texto**: O prompt de texto é convertido em embeddings usando um modelo CLIP (Contrastive Language-Image Pre-training)\n2. **Geração de Ruído Latente**: Começa-se com ruído aleatório no espaço latente (comprimido)\n3. **Processo de Difusão**: O modelo U-Net remove iterativamente o ruído, guiado pelos embeddings de texto\n4. **Decodificação**: Um VAE (Variational Autoencoder) converte a representação latente em uma imagem de alta resolução\n\n### Objetivos deste Projeto\n\nNeste notebook, vamos:\n\n- Explorar a arquitetura do Stable Diffusion usando a biblioteca `diffusers` da Hugging Face\n- Entender os principais componentes: CLIP, U-Net, VAE e Scheduler\n- Analisar o impacto de diferentes hiperparâmetros na qualidade das imagens geradas\n- Experimentar com text-to-image e image-to-image generation\n- Demonstrar técnicas de otimização para uso eficiente de memória\n\n### Requisitos\n\n- Python 3.8+\n- PyTorch com suporte CUDA (recomendado) ou CPU\n- Biblioteca `diffusers` da Hugging Face\n- ~4-6 GB de VRAM (GPU) ou ~8 GB de RAM (CPU)\n- Conexão à internet para download dos modelos pré-treinados",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "iif1xmqbm8",
   "source": "## 1. Configuração do Ambiente\n\nNesta seção, vamos configurar o ambiente necessário para executar o Stable Diffusion. Isso inclui:\n\n- **Importação de bibliotecas**: PyTorch, Diffusers, e ferramentas de visualização\n- **Verificação de hardware**: Detectar se temos GPU disponível (CUDA)\n- **Configuração de device**: Usar GPU se disponível, caso contrário CPU\n\n> **Nota sobre performance**: \n> - Com GPU (CUDA): ~10-30 segundos por imagem\n> - Com CPU: ~5-10 minutos por imagem\n> \n> Para melhor experiência, recomenda-se usar Google Colab com GPU gratuita.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ebb495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "GPU disponível?: CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar se CUDA está disponível\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "print(f\"GPU disponível?: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6bafa",
   "metadata": {},
   "source": "## 2. Carregamento do Modelo Stable Diffusion\n\nAgora vamos carregar o modelo **Stable Diffusion v1.5** da Runway ML. Este é um dos modelos mais populares e está disponível gratuitamente.\n\n### Características do Modelo v1.5:\n\n- **Resolução padrão**: 512x512 pixels\n- **Tamanho do modelo**: ~4 GB\n- **Parâmetros totais**: ~860 milhões\n- **Licença**: CreativeML OpenRAIL-M (uso livre com restrições éticas)\n\n### Otimizações Aplicadas:\n\n1. **torch_dtype**: Usamos `float16` em GPU para reduzir uso de memória pela metade\n2. **safety_checker=None**: Desabilitamos o filtro de conteúdo para economizar memória (use com responsabilidade)\n3. **attention_slicing**: Permite processar atenção em blocos menores (útil para GPUs com pouca VRAM)\n\n> **Primeira execução**: O modelo será baixado do Hugging Face Hub (~4 GB). Execuções subsequentes usarão o cache local."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f975ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 14 files:   7%|▋         | 1/14 [00:00<00:10,  1.26it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Carregando o modelo Stable Diffusion\n",
    "# Usando o modelo v1.5 que é gratuito e open-source\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Configurações para otimizar memória\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    safety_checker=None,  # Desabilitar para economizar memória\n",
    "    requires_safety_checker=False\n",
    ")\n",
    "\n",
    "# Mover para GPU se disponível\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Habilitar otimizações de memória\n",
    "if torch.cuda.is_available():\n",
    "    pipe.enable_attention_slicing()\n",
    "    # pipe.enable_xformers_memory_efficient_attention()  # Descomente se xformers estiver instalado\n",
    "\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7zxureizo7k",
   "source": "## 3. Arquitetura do Stable Diffusion Pipeline\n\nO Stable Diffusion é composto por vários componentes que trabalham juntos. Vamos explorar cada um deles:\n\n### Componentes Principais:\n\n#### 1. **Text Encoder (CLIP)**\n- Converte texto em representações numéricas (embeddings)\n- Baseado no modelo CLIP da OpenAI\n- Produz vetores de 768 dimensões que capturam o significado semântico do prompt\n- Limitado a 77 tokens por prompt\n\n#### 2. **Tokenizer**\n- Processa o texto de entrada, dividindo-o em tokens\n- Vocabulário de ~49,000 tokens\n- Lida com palavras, subpalavras e caracteres especiais\n\n#### 3. **U-Net**\n- Coração do modelo de difusão\n- Arquitetura de encoder-decoder com conexões skip\n- Remove ruído iterativamente do espaço latente\n- ~860 milhões de parâmetros\n- Recebe como entrada: imagem com ruído + embeddings de texto\n\n#### 4. **VAE (Variational Autoencoder)**\n- **Encoder**: Comprime imagens 512x512 para espaço latente 64x64 (redução de 8x)\n- **Decoder**: Reconstrói imagens do espaço latente para pixels\n- Permite trabalhar com representações compactas, economizando memória e computação\n- 4 canais no espaço latente\n\n#### 5. **Scheduler**\n- Controla o processo de denoising\n- Define como o ruído é removido em cada step\n- Diferentes schedulers (DDPM, DDIM, DPM-Solver) afetam qualidade e velocidade\n- 1000 timesteps de treinamento, mas pode usar menos na inferência\n\n### Fluxo de Geração (Text-to-Image):\n\n```\nTexto → Tokenizer → Tokens → CLIP Encoder → Text Embeddings (768D)\n                                                      ↓\nRuído Aleatório (64x64x4) ─────────────→ U-Net (50 steps) → Latente Final\n                                                      ↓\n                                            VAE Decoder → Imagem (512x512)\n```\n\nA seguir, vamos executar uma função que exibe detalhes de cada componente:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização dos componentes do pipeline\n",
    "def explain_pipeline_architecture():\n",
    "    \"\"\"\n",
    "    Explica a arquitetura do Stable Diffusion Pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ARQUITETURA DO STABLE DIFFUSION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    components = {\n",
    "        \"1. Text Encoder (CLIP)\": {\n",
    "            \"Modelo\": pipe.text_encoder.__class__.__name__,\n",
    "            \"Função\": \"Converte texto em embeddings de 768 dimensões\",\n",
    "            \"Parâmetros\": sum(p.numel() for p in pipe.text_encoder.parameters())\n",
    "        },\n",
    "        \"2. Tokenizer\": {\n",
    "            \"Modelo\": pipe.tokenizer.__class__.__name__,\n",
    "            \"Função\": \"Tokeniza o texto de entrada (máx 77 tokens)\",\n",
    "            \"Vocab Size\": pipe.tokenizer.vocab_size\n",
    "        },\n",
    "        \"3. U-Net\": {\n",
    "            \"Modelo\": pipe.unet.__class__.__name__,\n",
    "            \"Função\": \"Modelo de difusão que remove ruído iterativamente\",\n",
    "            \"Parâmetros\": sum(p.numel() for p in pipe.unet.parameters()),\n",
    "            \"Input Channels\": pipe.unet.config.in_channels,\n",
    "            \"Output Channels\": pipe.unet.config.out_channels\n",
    "        },\n",
    "        \"4. VAE (Variational Autoencoder)\": {\n",
    "            \"Modelo\": pipe.vae.__class__.__name__,\n",
    "            \"Função\": \"Codifica/decodifica entre espaço latente e imagem\",\n",
    "            \"Latent Channels\": pipe.vae.config.latent_channels,\n",
    "            \"Parâmetros\": sum(p.numel() for p in pipe.vae.parameters())\n",
    "        },\n",
    "        \"5. Scheduler\": {\n",
    "            \"Modelo\": pipe.scheduler.__class__.__name__,\n",
    "            \"Função\": \"Controla o processo de denoising\",\n",
    "            \"Num Steps\": pipe.scheduler.config.num_train_timesteps\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for component, details in components.items():\n",
    "        print(f\"\\n{component}\")\n",
    "        print(\"-\" * 40)\n",
    "        for key, value in details.items():\n",
    "            print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FLUXO DO PROCESSO:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Texto → Tokenizer → Tokens\")\n",
    "    print(\"2. Tokens → Text Encoder (CLIP) → Text Embeddings\")\n",
    "    print(\"3. Random Noise + Text Embeddings → U-Net (iterativo)\")\n",
    "    print(\"4. U-Net realiza denoising em múltiplos steps\")\n",
    "    print(\"5. Latent Image → VAE Decoder → Imagem Final (512x512)\")\n",
    "    \n",
    "explain_pipeline_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jc8rphuzcz",
   "source": "### 5.3 Experimento 3: Controle de Estilos Artísticos\n\nUma das capacidades mais impressionantes do Stable Diffusion é gerar imagens no estilo de diferentes artistas ou movimentos artísticos, tudo através do prompt textual.\n\n**Técnica de Prompt Engineering:**\n\nAdicionamos modificadores de estilo ao prompt base:\n- \"photorealistic, 8k photography\" → Fotografia realista\n- \"oil painting in the style of Van Gogh\" → Pintura impressionista\n- \"japanese anime style, studio ghibli\" → Estilo anime\n- \"pencil sketch, detailed drawing\" → Desenho a lápis\n- \"watercolor painting, soft colors\" → Aquarela\n\n**Por que funciona?**\n\nO modelo CLIP foi treinado com milhões de pares imagem-texto da internet, aprendendo associações entre descrições textuais e características visuais. Ele \"entende\" conceitos como \"Van Gogh\", \"anime\", \"fotografia\", etc.\n\n**Dica:** Seja específico! Em vez de \"painting\", use \"oil painting\" ou \"watercolor painting\".",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "pisl74y5me",
   "source": "## 4. Função de Geração de Imagens\n\nAgora vamos criar uma função auxiliar que facilita a geração de imagens com diferentes parâmetros.\n\n### Parâmetros Importantes:\n\n- **prompt** (str): Descrição textual do que você quer gerar\n  - Seja específico e detalhado\n  - Pode incluir estilo artístico, iluminação, composição, etc.\n  - Exemplo: \"A majestic lion wearing a crown, digital art, highly detailed, 4k\"\n\n- **negative_prompt** (str, opcional): O que você NÃO quer na imagem\n  - Ajuda a evitar características indesejadas\n  - Exemplo: \"blurry, low quality, distorted, ugly, bad anatomy\"\n\n- **num_inference_steps** (int, 20-100): Número de iterações de denoising\n  - Mais steps = melhor qualidade, mas mais lento\n  - 20-30 steps: rápido, qualidade aceitável\n  - 40-50 steps: bom equilíbrio qualidade/velocidade\n  - 75-100 steps: máxima qualidade, muito lento\n\n- **guidance_scale** (float, 1-20): Força de aderência ao prompt\n  - Valores baixos (1-5): mais criativo, mas pode ignorar o prompt\n  - Valores médios (7-9): equilíbrio recomendado\n  - Valores altos (10-20): segue o prompt rigidamente, pode ficar saturado\n\n- **seed** (int, opcional): Semente para reprodutibilidade\n  - Mesma seed + mesmos parâmetros = mesma imagem\n  - Útil para comparações e debugging\n\n- **height/width** (int, múltiplos de 8): Dimensões da imagem\n  - Padrão: 512x512 (treinado para isso)\n  - Outras resoluções funcionam, mas podem ter qualidade inferior",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c95048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(\n",
    "    prompt: str,\n",
    "    negative_prompt: Optional[str] = None,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: float = 7.5,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    seed: Optional[int] = None,\n",
    "    num_images: int = 1\n",
    ") -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Gera imagens usando Stable Diffusion\n",
    "    \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    prompt: Descrição textual da imagem desejada\n",
    "    negative_prompt: O que evitar na geração\n",
    "    num_inference_steps: Número de passos de denoising (20-100)\n",
    "    guidance_scale: Força de aderência ao prompt (1-20)\n",
    "    height/width: Dimensões da imagem (múltiplos de 8)\n",
    "    seed: Semente para reprodutibilidade\n",
    "    num_images: Número de imagens a gerar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurar seed se fornecido\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "    \n",
    "    # Gerar imagens\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        generator=generator,\n",
    "        num_images_per_prompt=num_images\n",
    "    ).images\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Função auxiliar para visualizar resultados\n",
    "def plot_images(images: List[Image.Image], prompt: str, params: dict = None):\n",
    "    \"\"\"Visualiza as imagens geradas com seus parâmetros\"\"\"\n",
    "    n_images = len(images)\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(6*n_images, 6))\n",
    "    \n",
    "    if n_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (ax, img) in enumerate(zip(axes, images)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if idx == 0 and params:\n",
    "            title = f\"Prompt: {prompt[:50]}...\\n\"\n",
    "            title += f\"Steps: {params.get('steps', 'N/A')}, \"\n",
    "            title += f\"Guidance: {params.get('guidance', 'N/A')}, \"\n",
    "            title += f\"Seed: {params.get('seed', 'Random')}\"\n",
    "            ax.set_title(title, fontsize=10, pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35xrt0oa6ak",
   "source": "## 5. Experimentos: Análise de Hiperparâmetros\n\nAgora vamos realizar uma série de experimentos para entender como diferentes parâmetros afetam a qualidade e características das imagens geradas.\n\n### 5.1 Experimento 1: Impacto do Guidance Scale\n\nO **Guidance Scale** (também chamado de Classifier-Free Guidance) controla o quanto o modelo deve seguir o prompt textual.\n\n**O que esperamos observar:**\n\n- **Guidance baixo (2-5)**: Imagens mais criativas e variadas, mas podem não seguir o prompt fielmente\n- **Guidance médio (7-9)**: Equilíbrio entre criatividade e fidelidade ao prompt\n- **Guidance alto (10-15)**: Imagens muito fiéis ao prompt, mas podem ficar supersaturadas ou artificiais\n\n**Por que isso acontece?**\n\nO guidance scale amplifica a diferença entre a predição condicionada (com prompt) e não-condicionada (sem prompt). Valores altos forçam o modelo a seguir mais rigidamente as instruções do texto.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Variando Guidance Scale\n",
    "print(\"EXEMPLO 1: Efeito do Guidance Scale na Geração\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "prompt = \"A majestic lion wearing a crown, digital art, highly detailed\"\n",
    "negative_prompt = \"blurry, low quality, distorted\"\n",
    "\n",
    "guidance_scales = [2.0, 5.0, 7.5, 10.0, 15.0]\n",
    "images_guidance = []\n",
    "\n",
    "for guidance in guidance_scales:\n",
    "    print(f\"Gerando com guidance_scale={guidance}...\")\n",
    "    img = generate_images(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=guidance,\n",
    "        num_inference_steps=30,\n",
    "        seed=42  # Mesma seed para comparação\n",
    "    )[0]\n",
    "    images_guidance.append(img)\n",
    "\n",
    "# Plotar resultados\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for idx, (img, guidance) in enumerate(zip(images_guidance, guidance_scales)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Guidance: {guidance}\")\n",
    "    axes[idx].axis('off')\n",
    "plt.suptitle(\"Impacto do Guidance Scale (maior = mais fiel ao prompt)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l5q2lf8g3r",
   "source": "### 5.2 Experimento 2: Número de Inference Steps (Passos de Denoising)\n\nO **número de steps** determina quantas iterações o modelo U-Net faz para remover o ruído da imagem latente.\n\n**O que esperamos observar:**\n\n- **Poucos steps (10-20)**: Geração rápida, mas imagens podem ter menos detalhes ou artefatos\n- **Steps médios (30-50)**: Bom equilíbrio entre qualidade e tempo\n- **Muitos steps (75-100)**: Máxima qualidade e refinamento, mas retorno diminui (lei de rendimentos decrescentes)\n\n**Processo de Denoising:**\n\nO modelo começa com ruído puro e gradualmente refina a imagem:\n- Step 1-10: Forma geral e composição\n- Step 11-30: Detalhes principais e estrutura\n- Step 31-50: Refinamento de texturas e detalhes finos\n- Step 51-100: Ajustes sutis (ganho marginal)\n\n**Trade-off:** Mais steps = melhor qualidade, mas tempo de geração aumenta linearmente.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: Variando número de inference steps\n",
    "print(\"EXEMPLO 2: Efeito do Número de Steps de Denoising\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "prompt = \"A cyberpunk city at night with neon lights, rainy weather, reflections\"\n",
    "steps_list = [10, 20, 30, 50, 75]\n",
    "images_steps = []\n",
    "\n",
    "for steps in steps_list:\n",
    "    print(f\"Gerando com {steps} steps...\")\n",
    "    img = generate_images(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=7.5,\n",
    "        seed=123  # Mesma seed\n",
    "    )[0]\n",
    "    images_steps.append(img)\n",
    "\n",
    "# Plotar\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for idx, (img, steps) in enumerate(zip(images_steps, steps_list)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Steps: {steps}\")\n",
    "    axes[idx].axis('off')\n",
    "plt.suptitle(\"Impacto do Número de Steps (mais steps = mais refinamento)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 3: Diferentes estilos artísticos\n",
    "print(\"EXEMPLO 3: Gerando Diferentes Estilos Artísticos\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "base_subject = \"a beautiful mountain landscape with a lake\"\n",
    "styles = [\n",
    "    \"photorealistic, 8k photography\",\n",
    "    \"oil painting in the style of Van Gogh\",\n",
    "    \"japanese anime style, studio ghibli\",\n",
    "    \"pencil sketch, detailed drawing\",\n",
    "    \"watercolor painting, soft colors\"\n",
    "]\n",
    "\n",
    "images_styles = []\n",
    "for style in styles:\n",
    "    full_prompt = f\"{base_subject}, {style}\"\n",
    "    print(f\"Gerando: {style[:30]}...\")\n",
    "    img = generate_images(\n",
    "        prompt=full_prompt,\n",
    "        num_inference_steps=40,\n",
    "        guidance_scale=8.0\n",
    "    )[0]\n",
    "    images_styles.append(img)\n",
    "\n",
    "# Plotar\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for idx, (img, style) in enumerate(zip(images_styles, styles)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(style[:30] + \"...\", fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "plt.suptitle(\"Mesmo Tema em Diferentes Estilos Artísticos\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "omcw1vg1i7m",
   "source": "### 5.4 Experimento 4: Poder do Negative Prompt\n\nO **negative prompt** é uma ferramenta crucial para melhorar a qualidade das imagens, permitindo especificar o que você NÃO quer na geração.\n\n**Como funciona tecnicamente?**\n\nDurante o processo de Classifier-Free Guidance, o modelo calcula:\n```\nnoise_pred = noise_unconditional + guidance_scale * (noise_conditional - noise_unconditional)\n```\n\nCom negative prompt, isso se torna:\n```\nnoise_pred = noise_negative + guidance_scale * (noise_conditional - noise_negative)\n```\n\nIsso \"empurra\" a geração para longe dos conceitos negativos.\n\n**Negative Prompts Comuns:**\n\n- **Qualidade geral**: \"blurry, low quality, low resolution, pixelated\"\n- **Anatomia (pessoas)**: \"bad anatomy, extra limbs, deformed, disfigured\"\n- **Artefatos**: \"watermark, text, signature, logo\"\n- **Estilo indesejado**: \"cartoon, anime\" (se você quer realismo)\n- **Iluminação**: \"dark, poorly lit, overexposed\"\n\n**Estratégia:** Comece simples e adicione termos negativos conforme identifica problemas nas gerações.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52612f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 4: Impacto do Negative Prompt\n",
    "print(\"EXEMPLO 4: Importância do Negative Prompt\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "prompt = \"A portrait of a wizard casting a spell, fantasy art\"\n",
    "\n",
    "negative_prompts = [\n",
    "    \"\",  # Sem negative prompt\n",
    "    \"ugly, distorted\",\n",
    "    \"ugly, distorted, blurry, low quality\",\n",
    "    \"ugly, distorted, blurry, low quality, extra limbs, bad anatomy\",\n",
    "    \"ugly, distorted, blurry, low quality, extra limbs, bad anatomy, cartoon, anime\"\n",
    "]\n",
    "\n",
    "images_negative = []\n",
    "for neg_prompt in negative_prompts:\n",
    "    print(f\"Negative prompt: {neg_prompt[:30] if neg_prompt else 'None'}...\")\n",
    "    img = generate_images(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=neg_prompt if neg_prompt else None,\n",
    "        num_inference_steps=40,\n",
    "        guidance_scale=7.5,\n",
    "        seed=999\n",
    "    )[0]\n",
    "    images_negative.append(img)\n",
    "\n",
    "# Plotar\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for idx, (img, neg) in enumerate(zip(images_negative, negative_prompts)):\n",
    "    axes[idx].imshow(img)\n",
    "    title = neg[:20] + \"...\" if neg else \"Sem negative\"\n",
    "    axes[idx].set_title(title, fontsize=9)\n",
    "    axes[idx].axis('off')\n",
    "plt.suptitle(\"Efeito do Negative Prompt na Qualidade\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qitl8z2085",
   "source": "### 5.5 Experimento 5: Reprodutibilidade com Seeds\n\nA **seed** (semente) controla a geração de números aleatórios, permitindo reproduzir exatamente a mesma imagem.\n\n**Como funciona:**\n\n1. A seed inicializa o gerador de ruído aleatório\n2. O ruído inicial latente (64x64x4) é gerado pseudo-aleatoriamente\n3. Mesma seed + mesmos parâmetros = mesmo ruído inicial = mesma imagem final\n\n**Aplicações práticas:**\n\n- **Comparações**: Testar o efeito de mudar apenas um parâmetro (como guidance scale)\n- **Refinamento**: Gerar várias versões com seeds diferentes, escolher a melhor, e regenerar com ajustes\n- **Reprodução**: Compartilhar seeds com outros para recriar imagens exatas\n- **Debugging**: Isolar problemas mantendo tudo constante exceto uma variável\n\n**Observação:** Sem especificar seed, cada geração será única e aleatória.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccf0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 5: Variação com diferentes seeds\n",
    "print(\"EXEMPLO 5: Variação com Diferentes Seeds\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "prompt = \"A futuristic robot in a garden, detailed, artistic\"\n",
    "seeds = [42, 123, 456, 789, 2024]\n",
    "images_seeds = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Gerando com seed={seed}...\")\n",
    "    img = generate_images(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=35,\n",
    "        guidance_scale=7.5,\n",
    "        seed=seed\n",
    "    )[0]\n",
    "    images_seeds.append(img)\n",
    "\n",
    "# Plotar\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for idx, (img, seed) in enumerate(zip(images_seeds, seeds)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Seed: {seed}\")\n",
    "    axes[idx].axis('off')\n",
    "plt.suptitle(\"Variações do Mesmo Prompt com Seeds Diferentes\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fehxr4kz78t",
   "source": "## 6. Análise de Performance\n\nÉ importante entender o trade-off entre qualidade e tempo de geração para otimizar seu workflow.\n\n**Fatores que afetam o tempo:**\n\n1. **Hardware**: GPU vs CPU (diferença de 10-30x)\n2. **Número de steps**: Relação linear (50 steps ≈ 2x mais lento que 25 steps)\n3. **Resolução**: Imagens maiores levam muito mais tempo\n4. **Batch size**: Gerar múltiplas imagens simultaneamente é mais eficiente\n\n**Benchmarks típicos (GPU RTX 3060, 50 steps, 512x512):**\n\n- Stable Diffusion v1.5: ~3-5 segundos por imagem\n- Stable Diffusion v2.1: ~4-6 segundos por imagem\n- Stable Diffusion XL: ~10-15 segundos por imagem\n\n**Otimizações aplicáveis:**\n\n- `enable_attention_slicing()`: Reduz uso de VRAM\n- `enable_vae_slicing()`: Processa VAE em tiles menores\n- `enable_xformers_memory_efficient_attention()`: Requer biblioteca xformers\n- Float16 vs Float32: Reduz VRAM pela metade",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_generation(steps_list=[20, 30, 50]):\n",
    "    \"\"\"Analisa tempo de geração vs qualidade\"\"\"\n",
    "    print(\"ANÁLISE DE PERFORMANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    prompt = \"A detailed portrait of a astronaut, professional photography\"\n",
    "    results = []\n",
    "    \n",
    "    for steps in steps_list:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        img = generate_images(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=steps,\n",
    "            seed=42\n",
    "        )[0]\n",
    "        \n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'steps': steps,\n",
    "            'time': gen_time,\n",
    "            'time_per_step': gen_time / steps,\n",
    "            'image': img\n",
    "        })\n",
    "        \n",
    "        print(f\"Steps: {steps:3d} | Tempo: {gen_time:.2f}s | Por step: {gen_time/steps:.3f}s\")\n",
    "    \n",
    "    # Plotar resultados\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(15, 5))\n",
    "    for idx, res in enumerate(results):\n",
    "        axes[idx].imshow(res['image'])\n",
    "        axes[idx].set_title(\n",
    "            f\"Steps: {res['steps']}\\n\"\n",
    "            f\"Tempo: {res['time']:.1f}s\\n\"\n",
    "            f\"ms/step: {res['time_per_step']*1000:.1f}\",\n",
    "            fontsize=10\n",
    "        )\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Trade-off: Qualidade vs Tempo de Geração\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Executar benchmark\n",
    "results = benchmark_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tyjdnjg3ag",
   "source": "## 7. Salvamento e Organização de Imagens Geradas\n\nPara projetos maiores, é útil ter um sistema de salvamento organizado com metadados.\n\n**Sistema de Salvamento:**\n\nA função `save_generation_batch` implementa:\n\n1. **Organização**: Cria diretório `generated_images/` automaticamente\n2. **Timestamp**: Adiciona data/hora aos nomes dos arquivos para evitar sobrescrita\n3. **Metadados JSON**: Salva todos os parâmetros usados na geração\n4. **Rastreabilidade**: Permite reproduzir exatamente qualquer imagem\n\n**Estrutura dos metadados:**\n\n```json\n{\n  \"name\": \"landscape\",\n  \"file\": \"20240115_143022_landscape.png\",\n  \"config\": {\n    \"prompt\": \"Beautiful mountain landscape at sunset\",\n    \"num_inference_steps\": 40,\n    \"guidance_scale\": 7.5,\n    \"seed\": 42\n  }\n}\n```\n\n**Benefícios:**\n\n- Documentação automática de experimentos\n- Facilita comparações entre diferentes configurações\n- Permite recriar imagens bem-sucedidas\n- Útil para criar datasets ou portfolios",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ef23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_generation_batch(prompts_dict, output_dir=\"generated_images\"):\n",
    "    \"\"\"\n",
    "    Salva um batch de gerações com metadados\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    for name, config in prompts_dict.items():\n",
    "        print(f\"Gerando: {name}...\")\n",
    "        \n",
    "        img = generate_images(**config)[0]\n",
    "        \n",
    "        filename = f\"{timestamp}_{name}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        img.save(filepath)\n",
    "        \n",
    "        metadata.append({\n",
    "            'name': name,\n",
    "            'file': filename,\n",
    "            'config': config\n",
    "        })\n",
    "        \n",
    "        print(f\"  Salvo em: {filepath}\")\n",
    "    \n",
    "    # Salvar metadados\n",
    "    import json\n",
    "    metadata_file = os.path.join(output_dir, f\"{timestamp}_metadata.json\")\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nMetadados salvos em: {metadata_file}\")\n",
    "    return metadata\n",
    "\n",
    "# Exemplo de uso\n",
    "prompts_para_salvar = {\n",
    "    \"landscape\": {\n",
    "        \"prompt\": \"Beautiful mountain landscape at sunset, photorealistic\",\n",
    "        \"num_inference_steps\": 40,\n",
    "        \"guidance_scale\": 7.5,\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"portrait\": {\n",
    "        \"prompt\": \"Professional portrait of a scientist in laboratory\",\n",
    "        \"num_inference_steps\": 50,\n",
    "        \"guidance_scale\": 8.0,\n",
    "        \"seed\": 123\n",
    "    },\n",
    "    \"abstract\": {\n",
    "        \"prompt\": \"Abstract colorful geometric patterns, modern art\",\n",
    "        \"num_inference_steps\": 35,\n",
    "        \"guidance_scale\": 6.0,\n",
    "        \"seed\": 456\n",
    "    }\n",
    "}\n",
    "\n",
    "# Descomente para salvar\n",
    "# metadata = save_generation_batch(prompts_para_salvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hu4jhjtascu",
   "source": "## 8. Resumo e Estatísticas do Modelo\n\nEsta seção apresenta um resumo completo de tudo que foi implementado e explorado neste projeto.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15313971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RESUMO DO PROJETO - STABLE DIFFUSION COM DIFFUSERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = \"\"\"\n",
    "IMPLEMENTAÇÕES REALIZADAS:\n",
    "--------------------------\n",
    "1. TEXT-TO-IMAGE: Pipeline principal com Stable Diffusion v1.5\n",
    "2. IMAGE-TO-IMAGE: Transformação de imagens existentes com prompts\n",
    "\n",
    "ARQUITETURA EXPLORADA:\n",
    "----------------------\n",
    "- Text Encoder (CLIP): Converte prompts em embeddings semânticos\n",
    "- U-Net: Realiza o processo de difusão/denoising iterativo\n",
    "- VAE: Codifica/decodifica entre espaço latente e pixels\n",
    "- Scheduler: Controla o processo de remoção de ruído\n",
    "\n",
    "PARÂMETROS ANALISADOS:\n",
    "----------------------\n",
    "- Guidance Scale: Controla fidelidade ao prompt (2-15)\n",
    "- Inference Steps: Número de iterações de denoising (20-100)\n",
    "- Strength (img2img): Intensidade da transformação (0-1)\n",
    "- Seed: Controle de reprodutibilidade\n",
    "- Negative Prompt: Elementos a evitar na geração\n",
    "\n",
    "EXEMPLOS DEMONSTRADOS:\n",
    "----------------------\n",
    "✓ 5+ variações de guidance scale\n",
    "✓ 5+ variações de inference steps\n",
    "✓ 5+ estilos artísticos diferentes\n",
    "✓ 5+ exemplos de negative prompts\n",
    "✓ 5+ seeds diferentes\n",
    "✓ 5+ transformações image-to-image\n",
    "\n",
    "OTIMIZAÇÕES APLICADAS:\n",
    "----------------------\n",
    "- Float16 para economia de memória\n",
    "- Attention slicing para GPUs com menos VRAM\n",
    "- Cache de modelos para reutilização\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Estatísticas finais\n",
    "total_params = sum(p.numel() for p in pipe.unet.parameters())\n",
    "total_params += sum(p.numel() for p in pipe.vae.parameters())\n",
    "total_params += sum(p.numel() for p in pipe.text_encoder.parameters())\n",
    "\n",
    "print(f\"\\nTOTAL DE PARÂMETROS NO MODELO: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "print(f\"MEMÓRIA GPU UTILIZADA: ~4-6 GB em float16\")\n",
    "print(f\"TEMPO MÉDIO POR IMAGEM (50 steps): ~10-30 segundos (varia com GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dmg78q21dr",
   "source": "## 9. Conclusões e Aprendizados\n\n### Principais Descobertas\n\nAtravés dos experimentos realizados neste projeto, aprendemos que:\n\n#### 1. **Qualidade vs Performance**\n- Existe um ponto ideal de equilíbrio: **40-50 steps** com **guidance scale 7.5-8.0**\n- Além de 50 steps, os ganhos de qualidade são marginais\n- Guidance scale acima de 12 tende a produzir imagens supersaturadas\n\n#### 2. **Importância do Prompt Engineering**\n- Prompts específicos e detalhados geram resultados muito melhores\n- Modificadores de estilo (\"oil painting\", \"photorealistic\") são extremamente eficazes\n- Negative prompts são essenciais para evitar artefatos comuns\n\n#### 3. **Arquitetura do Modelo**\n- O VAE reduz a computação em 64x ao trabalhar em espaço latente (64x64 vs 512x512)\n- O U-Net é o componente mais pesado (~860M parâmetros)\n- CLIP conecta linguagem e visão de forma surpreendentemente eficaz\n\n#### 4. **Reprodutibilidade**\n- Seeds permitem controle total sobre gerações\n- Útil para debugging e comparações científicas\n- Pequenas mudanças no prompt podem causar grandes mudanças na saída\n\n### Aplicações Práticas\n\nO Stable Diffusion pode ser usado para:\n\n- **Arte e Design**: Concept art, ilustrações, designs de produtos\n- **Marketing**: Geração de imagens para campanhas, ads, social media\n- **Prototipagem**: Mockups visuais, exploração de ideias\n- **Educação**: Visualização de conceitos abstratos\n- **Pesquisa**: Estudos sobre IA generativa, bias, e representação\n\n### Limitações Observadas\n\n- **Anatomia humana**: Ainda gera erros em mãos, dedos, e posturas complexas\n- **Texto em imagens**: Geralmente produz texto ilegível ou incorreto\n- **Detalhes finos**: Pequenos objetos ou padrões complexos podem ser inconsistentes\n- **Bias**: Reflete biases presentes nos dados de treinamento\n\n### Próximos Passos\n\nPara expandir este projeto, considere:\n\n1. **Image-to-Image**: Transformar imagens existentes com prompts\n2. **Inpainting**: Editar partes específicas de imagens\n3. **ControlNet**: Controle mais preciso com mapas de profundidade, edges, poses\n4. **LoRA**: Fine-tuning eficiente para estilos específicos\n5. **Stable Diffusion XL**: Versão mais recente com melhor qualidade\n\n### Recursos Adicionais\n\n- [Documentação Diffusers](https://huggingface.co/docs/diffusers)\n- [Stable Diffusion Paper](https://arxiv.org/abs/2112.10752)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n\n---\n\n**Projeto desenvolvido para o curso de Redes Neurais Artificiais**  \n*Demonstrando compreensão de modelos de difusão latente e suas aplicações práticas*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}